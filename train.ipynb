{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/souravraha/Progressive-GAN-pytorch/blob/lightning/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93477e7c"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import argparse\n",
        "import random"
      ],
      "id": "93477e7c",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cec3df4"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable, grad\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, utils"
      ],
      "id": "8cec3df4",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cf164c0"
      },
      "source": [
        "from progan_modules import Generator, Discriminator"
      ],
      "id": "8cf164c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feb8407c"
      },
      "source": [
        "def accumulate(model1, model2, decay=0.999):\n",
        "    par1 = dict(model1.named_parameters())\n",
        "    par2 = dict(model2.named_parameters())\n",
        "\n",
        "    for k in par1.keys():\n",
        "        par1[k].data.mul_(decay).add_(1 - decay, par2[k].data)"
      ],
      "id": "feb8407c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9b92335"
      },
      "source": [
        "def imagefolder_loader(path):\n",
        "    def npy_loader(path):\n",
        "        # s=np.load(path).astype('float',copy=False)\n",
        "        return torch.from_numpy(np.load(path)).unsqueeze(0).float()\n",
        "\n",
        "    def loader(transform):\n",
        "        data = datasets.DatasetFolder(path, npy_loader, ('.npy'), transform=transform)\n",
        "        data_loader = DataLoader(data, shuffle=True, batch_size=batch_size,\n",
        "                                 num_workers=4)\n",
        "        return data_loader\n",
        "    return loader"
      ],
      "id": "d9b92335",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce63205e"
      },
      "source": [
        "def sample_data(dataloader, image_size=4):\n",
        "    GLOBAL = np.load('/content/GLOBAL_VALS_F.npz')\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Normalize((GLOBAL['VALS'][2]), (GLOBAL['VALS'][3])),\n",
        "        transforms.Resize(image_size, transforms.InterpolationMode.NEAREST),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "    ])\n",
        "\n",
        "    loader = dataloader(transform)\n",
        "\n",
        "    return loader"
      ],
      "id": "ce63205e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "9b50d80e"
      },
      "source": [
        "def train(generator, discriminator, init_step, loader, total_iter=600000):\n",
        "    step = init_step # can be 1 = 8, 2 = 16, 3 = 32, 4 = 64, 5 = 128, 6 = 128\n",
        "    data_loader = sample_data(loader, 4 * 2 ** step)\n",
        "    dataset = iter(data_loader)\n",
        "\n",
        "    #total_iter = 600000\n",
        "    total_iter_remain = total_iter - (total_iter//6)*(step-1)\n",
        "\n",
        "    pbar = tqdm(range(total_iter_remain))\n",
        "\n",
        "    disc_loss_val = 0\n",
        "    gen_loss_val = 0\n",
        "    grad_loss_val = 0\n",
        "\n",
        "    from datetime import datetime\n",
        "    import os\n",
        "    date_time = datetime.now()\n",
        "    post_fix = '%s_%s_%d_%d.txt'%(trial_name, date_time.date(), date_time.hour, date_time.minute)\n",
        "    log_folder = 'trial_%s_%s_%d_%d'%(trial_name, date_time.date(), date_time.hour, date_time.minute)\n",
        "    \n",
        "    os.mkdir(log_folder)\n",
        "    os.mkdir(log_folder+'/checkpoint')\n",
        "    os.mkdir(log_folder+'/sample')\n",
        "\n",
        "    config_file_name = os.path.join(log_folder, 'train_config_'+post_fix)\n",
        "    config_file = open(config_file_name, 'w')\n",
        "    config_file.write(str(args))\n",
        "    config_file.close()\n",
        "\n",
        "    log_file_name = os.path.join(log_folder, 'train_log_'+post_fix)\n",
        "    log_file = open(log_file_name, 'w')\n",
        "    log_file.write('g,d,nll,onehot\\n')\n",
        "    log_file.close()\n",
        "\n",
        "    from shutil import copy\n",
        "    copy('train.py', log_folder+'/train_%s.py'%post_fix)\n",
        "    copy('progan_modules.py', log_folder+'/model_%s.py'%post_fix)\n",
        "\n",
        "    alpha = 0\n",
        "    #one = torch.FloatTensor([1]).to(device)\n",
        "    one = torch.tensor(1, dtype=torch.float).to(device)\n",
        "    mone = one * -1\n",
        "    iteration = 0\n",
        "\n",
        "    for i in pbar:\n",
        "        discriminator.zero_grad()\n",
        "\n",
        "        alpha = min(1, (2/(total_iter//6)) * iteration)\n",
        "\n",
        "        if iteration > total_iter//6:\n",
        "            alpha = 0\n",
        "            iteration = 0\n",
        "            step += 1\n",
        "\n",
        "            if step > 6:\n",
        "                alpha = 1\n",
        "                step = 6\n",
        "            data_loader = sample_data(loader, 4 * 2 ** step)\n",
        "            dataset = iter(data_loader)\n",
        "\n",
        "        try:\n",
        "            real_image, label = next(dataset)\n",
        "\n",
        "        except (OSError, StopIteration):\n",
        "            dataset = iter(data_loader)\n",
        "            real_image, label = next(dataset)\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "        ### 1. train Discriminator\n",
        "        b_size = real_image.size(0)\n",
        "        real_image = real_image.to(device)\n",
        "        label = label.to(device)\n",
        "        real_predict = discriminator(\n",
        "            real_image, step=step, alpha=alpha)\n",
        "        real_predict = real_predict.mean() \\\n",
        "            - 0.001 * (real_predict ** 2).mean()\n",
        "        real_predict.backward(mone)\n",
        "\n",
        "        # sample input data: vector for Generator\n",
        "        gen_z = torch.randn(b_size, input_code_size).to(device)\n",
        "\n",
        "        fake_image = generator(gen_z, step=step, alpha=alpha)\n",
        "        fake_predict = discriminator(\n",
        "            fake_image.detach(), step=step, alpha=alpha)\n",
        "        fake_predict = fake_predict.mean()\n",
        "        fake_predict.backward(one)\n",
        "\n",
        "        ### gradient penalty for D\n",
        "        eps = torch.rand(b_size, 1, 1, 1).to(device)\n",
        "        x_hat = eps * real_image.data + (1 - eps) * fake_image.detach().data\n",
        "        x_hat.requires_grad = True\n",
        "        hat_predict = discriminator(x_hat, step=step, alpha=alpha)\n",
        "        grad_x_hat = grad(\n",
        "            outputs=hat_predict.sum(), inputs=x_hat, create_graph=True)[0]\n",
        "        grad_penalty = ((grad_x_hat.view(grad_x_hat.size(0), -1)\n",
        "                         .norm(2, dim=1) - 1)**2).mean()\n",
        "        grad_penalty = 10 * grad_penalty\n",
        "        grad_penalty.backward()\n",
        "        grad_loss_val += grad_penalty.item()\n",
        "        disc_loss_val += (real_predict - fake_predict).item()\n",
        "\n",
        "        d_optimizer.step()\n",
        "\n",
        "        ### 2. train Generator\n",
        "        if (i + 1) % n_critic == 0:\n",
        "            generator.zero_grad()\n",
        "            discriminator.zero_grad()\n",
        "            \n",
        "            predict = discriminator(fake_image, step=step, alpha=alpha)\n",
        "\n",
        "            loss = -predict.mean()\n",
        "            gen_loss_val += loss.item()\n",
        "\n",
        "\n",
        "            loss.backward()\n",
        "            g_optimizer.step()\n",
        "            accumulate(g_running, generator)\n",
        "\n",
        "        if (i + 1) % 1000 == 0 or i==0:\n",
        "            with torch.no_grad():\n",
        "                images = g_running(torch.randn(5 * 10, input_code_size).to(device), step=step, alpha=alpha).data.cpu()\n",
        "\n",
        "                utils.save_image(\n",
        "                    images,\n",
        "                    f'{log_folder}/sample/{str(i + 1).zfill(6)}.png',\n",
        "                    nrow=10,\n",
        "                    normalize=True,\n",
        "                    range=(-1, 1))\n",
        " \n",
        "        if (i+1) % 10000 == 0 or i==0:\n",
        "            try:\n",
        "                torch.save(g_running.state_dict(), f'{log_folder}/checkpoint/{str(i + 1).zfill(6)}_g.model')\n",
        "                torch.save(discriminator.state_dict(), f'{log_folder}/checkpoint/{str(i + 1).zfill(6)}_d.model')\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        if (i+1)%500 == 0:\n",
        "            state_msg = (f'{i + 1}; G: {gen_loss_val/(500//n_critic):.3f}; D: {disc_loss_val/500:.3f};'\n",
        "                f' Grad: {grad_loss_val/500:.3f}; Alpha: {alpha:.3f}')\n",
        "            \n",
        "            log_file = open(log_file_name, 'a+')\n",
        "            new_line = \"%.5f,%.5f\\n\"%(gen_loss_val/(500//n_critic), disc_loss_val/500)\n",
        "            log_file.write(new_line)\n",
        "            log_file.close()\n",
        "\n",
        "            disc_loss_val = 0\n",
        "            gen_loss_val = 0\n",
        "            grad_loss_val = 0\n",
        "\n",
        "            print(state_msg)\n",
        "            #pbar.set_description(state_msg)"
      ],
      "id": "9b50d80e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afb50887"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='Progressive GAN, during training, the model will learn to generate  images from a low resolution, then progressively getting high resolution ')\n",
        "\n",
        "    parser.add_argument('--path', type=str, help='path of specified dataset, should be a folder that has one or many sub image folders inside')\n",
        "    parser.add_argument('--trial_name', type=str, default=\"test1\", help='a brief description of the training trial')\n",
        "    parser.add_argument('--gpu_id', type=int, default=0, help='0 is the first gpu, 1 is the second gpu, etc.')\n",
        "    parser.add_argument('--lr', type=float, default=0.001, help='learning rate, default is 1e-3, usually dont need to change it, you can try make it bigger, such as 2e-3')\n",
        "    parser.add_argument('--z_dim', type=int, default=128, help='the initial latent vector\\'s dimension, can be smaller such as 64, if the dataset is not diverse')\n",
        "    parser.add_argument('--channel', type=int, default=128, help='determines how big the model is, smaller value means faster training, but less capacity of the model')\n",
        "    parser.add_argument('--batch_size', type=int, default=4, help='how many images to train together at one iteration')\n",
        "    parser.add_argument('--n_critic', type=int, default=1, help='train Dhow many times while train G 1 time')\n",
        "    parser.add_argument('--init_step', type=int, default=1, help='start from what resolution, 1 means 8x8 resolution, 2 means 16x16 resolution, ..., 6 means 256x256 resolution')\n",
        "    parser.add_argument('--total_iter', type=int, default=300000, help='how many iterations to train in total, the value is in assumption that init step is 1')\n",
        "    parser.add_argument('--pixel_norm', default=False, action=\"store_true\", help='a normalization method inside the model, you can try use it or not depends on the dataset')\n",
        "    parser.add_argument('--tanh', default=False, action=\"store_true\", help='an output non-linearity on the output of Generator, you can try use it or not depends on the dataset')\n",
        "    \n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(str(args))\n",
        "\n",
        "    trial_name = args.trial_name\n",
        "    device = torch.device(\"cuda:%d\"%(args.gpu_id))\n",
        "    input_code_size = args.z_dim\n",
        "    batch_size = args.batch_size\n",
        "    n_critic = args.n_critic\n",
        "\n",
        "    generator = Generator(in_channel=args.channel, input_code_dim=input_code_size, pixel_norm=args.pixel_norm, tanh=args.tanh).to(device)\n",
        "    discriminator = Discriminator(feat_dim=args.channel).to(device)\n",
        "    g_running = Generator(in_channel=args.channel, input_code_dim=input_code_size, pixel_norm=args.pixel_norm, tanh=args.tanh).to(device)\n",
        "    \n",
        "    ## you can directly load a pretrained model here\n",
        "    #generator.load_state_dict(torch.load('./tr checkpoint/150000_g.model'))\n",
        "    #g_running.load_state_dict(torch.load('checkpoint/150000_g.model'))\n",
        "    #discriminator.load_state_dict(torch.load('checkpoint/150000_d.model'))\n",
        "    \n",
        "    g_running.train(False)\n",
        "\n",
        "    g_optimizer = optim.Adam(generator.parameters(), lr=args.lr, betas=(0.0, 0.99))\n",
        "    d_optimizer = optim.Adam(discriminator.parameters(), lr=args.lr, betas=(0.0, 0.99))\n",
        "\n",
        "    accumulate(g_running, generator, 0)\n",
        "\n",
        "    loader = imagefolder_loader(args.path)\n",
        "\n",
        "    train(generator, discriminator, args.init_step, loader, args.total_iter)"
      ],
      "id": "afb50887",
      "execution_count": null,
      "outputs": []
    }
  ]
}